{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6768b9ac-576f-4ee5-8617-17ffe24e4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:46:23.837905: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-09 20:46:23.843387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-09 20:46:23.843407: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import urllib\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0550affb-d762-46c4-b4a6-e7cc085a9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
    "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
    "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c262a33a-c0b0-4e2a-b35e-a01e536c4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2094ef-d995-4d22-930b-2d44a6e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
    "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8766e6bf-020c-4e17-95dd-e2a4eb6631c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:46:32.017904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-09 20:46:32.017930: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-09 20:46:32.017950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lscr): /proc/driver/nvidia/version does not exist\n",
      "2022-09-09 20:46:32.018204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "download_and_extract_data()\n",
    "# Reads the dataset from the CSV.\n",
    "df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
    "                 infer_datetime_format=True, index_col='datetime', header=0)\n",
    "\n",
    "# Number of features in the dataset. We use all features as predictors to\n",
    "# predict all features at future time steps.\n",
    "N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
    "\n",
    "# Normalizes the data\n",
    "data = df.values\n",
    "data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
    "\n",
    "# Splits the data into training and validation sets.\n",
    "SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n",
    "x_train = data[:SPLIT_TIME]\n",
    "x_valid = data[SPLIT_TIME:]\n",
    "\n",
    "# DO NOT CHANGE THIS CODE\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
    "# THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
    "# In other cases, it is advised not to change the batch size since it\n",
    "# might affect your final scores. While setting it to a lower size\n",
    "# might not do any harm, higher sizes might affect your scores.\n",
    "BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
    "\n",
    "# DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
    "# on the server.\n",
    "# Number of past time steps based on which future observations should be\n",
    "# predicted\n",
    "N_PAST = 24  # DO NOT CHANGE THIS\n",
    "\n",
    "# Number of future time steps which are to be predicted.\n",
    "N_FUTURE = 24  # DO NOT CHANGE THIS\n",
    "\n",
    "# By how many positions the window slides to create a new window\n",
    "# of observations.\n",
    "SHIFT = 1  # DO NOT CHANGE THIS\n",
    "\n",
    "# Code to create windowed train and validation datasets.\n",
    "train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
    "                             n_past=N_PAST, n_future=N_FUTURE,\n",
    "                             shift=SHIFT)\n",
    "valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
    "                             n_past=N_PAST, n_future=N_FUTURE,\n",
    "                             shift=SHIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac464d0-f10b-4cae-834f-346851a076d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43200, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a23a49-0693-4d6c-b8d9-5eea68c24db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9d08d0-72fb-4717-a39d-548f2d558626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_model():\n",
    "    # Code to define your model.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # tf.keras.layers.Conv1D(filters=64,\n",
    "        #                        kernel_size=5,\n",
    "        #                        strides=1,\n",
    "        #                        activation=\"relu\",\n",
    "        #                        padding='causal',\n",
    "        #                        input_shape=(N_PAST, N_FEATURES)),\n",
    "        \n",
    "        tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(N_PAST, N_FEATURES)),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "\n",
    "        tf.keras.layers.Dense(N_FEATURES)\n",
    "    ])\n",
    "    # Code to train and compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam() # YOUR CODE HERE\n",
    "    model.compile(loss=tf.keras.losses.Huber(),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae']\n",
    "    )\n",
    "    print(model.summary)\n",
    "    history = model.fit(train_set,\n",
    "                        epochs=20,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data=valid_set\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6486c7-7f51-4a8a-b8d9-c428439624aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1349/1349 [==============================] - 41s 28ms/step - loss: 0.0127 - mae: 0.0942 - val_loss: 0.0107 - val_mae: 0.0794\n",
      "Epoch 2/20\n",
      "1349/1349 [==============================] - 37s 27ms/step - loss: 0.0105 - mae: 0.0800 - val_loss: 0.0101 - val_mae: 0.0746\n",
      "Epoch 3/20\n",
      "1349/1349 [==============================] - 37s 28ms/step - loss: 0.0101 - mae: 0.0756 - val_loss: 0.0097 - val_mae: 0.0690\n",
      "Epoch 4/20\n",
      "1349/1349 [==============================] - 38s 28ms/step - loss: 0.0099 - mae: 0.0735 - val_loss: 0.0096 - val_mae: 0.0680\n",
      "Epoch 5/20\n",
      "1349/1349 [==============================] - 38s 28ms/step - loss: 0.0098 - mae: 0.0726 - val_loss: 0.0096 - val_mae: 0.0684\n",
      "Epoch 6/20\n",
      "1349/1349 [==============================] - 37s 28ms/step - loss: 0.0098 - mae: 0.0723 - val_loss: 0.0095 - val_mae: 0.0680\n",
      "Epoch 7/20\n",
      "1349/1349 [==============================] - 39s 29ms/step - loss: 0.0097 - mae: 0.0718 - val_loss: 0.0095 - val_mae: 0.0679\n",
      "Epoch 8/20\n",
      "1349/1349 [==============================] - 38s 28ms/step - loss: 0.0097 - mae: 0.0715 - val_loss: 0.0094 - val_mae: 0.0675\n",
      "Epoch 9/20\n",
      "1349/1349 [==============================] - 24s 18ms/step - loss: 0.0096 - mae: 0.0712 - val_loss: 0.0095 - val_mae: 0.0684\n",
      "Epoch 10/20\n",
      "1349/1349 [==============================] - 21s 16ms/step - loss: 0.0096 - mae: 0.0710 - val_loss: 0.0094 - val_mae: 0.0682\n",
      "Epoch 11/20\n",
      "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0096 - mae: 0.0708 - val_loss: 0.0094 - val_mae: 0.0686\n",
      "Epoch 12/20\n",
      "1349/1349 [==============================] - 23s 17ms/step - loss: 0.0095 - mae: 0.0706 - val_loss: 0.0094 - val_mae: 0.0686\n",
      "Epoch 13/20\n",
      "1349/1349 [==============================] - 22s 17ms/step - loss: 0.0095 - mae: 0.0705 - val_loss: 0.0095 - val_mae: 0.0698\n",
      "Epoch 14/20\n",
      "1349/1349 [==============================] - 22s 16ms/step - loss: 0.0095 - mae: 0.0703 - val_loss: 0.0094 - val_mae: 0.0696\n",
      "Epoch 15/20\n",
      "1349/1349 [==============================] - 22s 16ms/step - loss: 0.0095 - mae: 0.0702 - val_loss: 0.0094 - val_mae: 0.0699\n",
      "Epoch 16/20\n",
      "1349/1349 [==============================] - 21s 16ms/step - loss: 0.0095 - mae: 0.0701 - val_loss: 0.0094 - val_mae: 0.0695\n",
      "Epoch 17/20\n",
      "1349/1349 [==============================] - 23s 17ms/step - loss: 0.0095 - mae: 0.0700 - val_loss: 0.0094 - val_mae: 0.0698\n",
      "Epoch 18/20\n",
      "1349/1349 [==============================] - 22s 16ms/step - loss: 0.0094 - mae: 0.0698 - val_loss: 0.0093 - val_mae: 0.0691\n",
      "Epoch 19/20\n",
      "1349/1349 [==============================] - 22s 17ms/step - loss: 0.0094 - mae: 0.0698 - val_loss: 0.0093 - val_mae: 0.0696\n",
      "Epoch 20/20\n",
      "1349/1349 [==============================] - 21s 16ms/step - loss: 0.0094 - mae: 0.0696 - val_loss: 0.0093 - val_mae: 0.0699\n"
     ]
    }
   ],
   "source": [
    "model, history = solution_model()\n",
    "model.save(\"c5q4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddacfbc-4a2f-479b-8e23-c69d4ebc36da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e9529-2b7b-43c9-be8c-60112c17fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
    "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
    "#def mae(y_true, y_pred):\n",
    "#    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
    "#\n",
    "#\n",
    "#def model_forecast(model, series, window_size, batch_size):\n",
    "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "#    forecast = model.predict(ds)\n",
    "#    return forecast\n",
    "#\n",
    "\n",
    "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
    "\n",
    "#rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
    "#rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :]\n",
    "\n",
    "#x_valid = x_valid[:rnn_forecast.shape[0]]\n",
    "#result = mae(x_valid, rnn_forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
